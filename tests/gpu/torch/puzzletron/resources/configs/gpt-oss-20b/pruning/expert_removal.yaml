defaults:
  - pruning_defaults

eval_samples: 10
activations_log_dir: ${puzzle_dir}/pruning/pruning_scores/expert_removal/${pruning.experiment_id}
pruning_mixin:
  _target_: modelopt.torch.puzzletron.pruning.expert_removal_pruning_mixin.ExpertRemovalPruningMixIn
  layer_descriptor:
    _target_: modelopt.torch.puzzletron.anymodel.models.gpt_oss_20b.gpt_oss_20b_model_descriptor.GptOss20bExpertRemovalLayerDescriptor

hook_class: ${get_object:modelopt.torch.nas.plugins.megatron_hooks.base_hooks.GptOssRemoveExpertsIndependentHook}
activation_hooks_kwargs:    # Additional kwargs to pass to the hook init

# num_experts_to_keep must be >= num_experts_per_tok (can't route to more experts than exist)
# Test model: num_local_experts=32, num_experts_per_tok=4
num_experts_to_keep_list: [24, 16, 8]
mlp_init_mode: "ExpertRemoval"
mlp_init_config_yaml:
  expert_scores_key: "expert_ranks_mse"
  layer_prefix_template: "model.layers.{layer_idx}."
